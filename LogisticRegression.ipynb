{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"align:center\"> Logistic Regression From Scratch on Skin Segmentation Data set</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the numpy library and load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "filename_train = \"Skin_NonSkin.txt\"\n",
    "filename_test = \"Skin_NonSkin_Test.txt\"\n",
    "data = np.genfromtxt(filename_train, dtype=np.float64)\n",
    "data1=np.genfromtxt(filename_test, dtype=np.float64)\n",
    "Y = data[:, 3]\n",
    "Y_test = data1[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple function to encode (One-Hot Encoding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode():\n",
    "    for i in range(0, len(Y)):\n",
    "        if Y[i] == 1:\n",
    "            Y[i] = 1.0\n",
    "        else:\n",
    "            Y[i] = 0.0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_test[i] == 1:\n",
    "            Y_test[i] = 1.0\n",
    "        else:\n",
    "            Y_test[i] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    max1 = max(data[:, 0])\n",
    "    min1 = min(data[:, 0])\n",
    "    max2 = max(data[:, 1])\n",
    "    max3 = max(data[:, 2])\n",
    "    min2 = min(data[:, 1])\n",
    "    min3 = min(data[:, 2])\n",
    "    data[:, 0] = (data[:, 0] - min1) / (max1 - min1)\n",
    "    data[:, 1] = (data[:, 1] - min2) / (max2 - min2)\n",
    "    data[:, 2] = (data[:, 2] - min3) / (max3 - min3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Hypothesis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hOFx(theta, x):\n",
    "    z = 0\n",
    "    for i in range(0, len(theta)):\n",
    "        z += x[i] * theta[i]\n",
    "    return sig(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    s= float(1.0 / float((1.0 + np.exp(-1.0 * z))))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(X, Y, theta, m):\n",
    "    err = 0\n",
    "    for i in range(m):\n",
    "        xi = X[i]\n",
    "        hi = hOFx(theta, xi)\n",
    "        e=(Y[i]*np.log(hi))+((1 - Y[i])*np.log(1 - hi))\n",
    "        err += e\n",
    "    J = (-1.0/m)*err\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of derivative of cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_cost(X, Y, theta, j, m, a):\n",
    "    err=0\n",
    "    for i in range(0, m):\n",
    "        hx=hOFx(theta,X[i])\n",
    "        e=(hx-Y[i])*X[i,j]\n",
    "        err+=e\n",
    "    m=len(Y)\n",
    "    J=float(a/m)*err\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, Y, theta, m, alpha):\n",
    "    t= []\n",
    "    for j in range(0, len(theta)):\n",
    "        cd = derivative_cost(X, Y, theta, j, m, alpha)\n",
    "        nj = theta[j] - cd\n",
    "        t.append(nj)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression(X, Y, a1, theta,iters):\n",
    "    m = len(Y)\n",
    "    encode()\n",
    "    normalize()\n",
    "    for x in range(0,iters):\n",
    "        nt= grad(X, Y, theta, m,a1)\n",
    "        theta = nt\n",
    "        if x % 100 == 0:\n",
    "            cost_func(X, Y, theta, m)\n",
    "            print(\"Theta=\", theta)\n",
    "            print(\"Cost=\", cost_func(X, Y, theta, m))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta= [-0.015255458589734051, -0.014050601846047906, -0.007551242441610384]\n",
      "Cost= 0.6883264150545672\n",
      "Theta= [-0.8169636848699138, -0.6625041251601019, -0.06811710372257088]\n",
      "Cost= 0.5577944551753007\n",
      "Theta= [-1.1812664274118307, -0.8618273297884026, 0.25516016940400627]\n",
      "Cost= 0.5297007453723476\n",
      "Theta= [-1.4642907760527926, -0.9874900216951243, 0.6087121900787088]\n",
      "Cost= 0.5075971689085714\n",
      "Theta= [-1.717549443969175, -1.0934333598460222, 0.9438444198462651]\n",
      "Cost= 0.4888267398446989\n",
      "Theta= [-1.9515448886494822, -1.1899611384182, 1.2539038362371488]\n",
      "Cost= 0.47280398037428917\n",
      "Theta= [-2.169370595171101, -1.2794506773326304, 1.5398822103684313]\n",
      "Cost= 0.4590785194056733\n",
      "Theta= [-2.3726703401005302, -1.362760505166809, 1.804042828226949]\n",
      "Cost= 0.4472722477723135\n",
      "Theta= [-2.5627497814599316, -1.440447311894005, 2.0486578503081603]\n",
      "Cost= 0.43707131003810773\n",
      "Theta= [-2.740775035433081, -1.512989950717077, 2.2757718368630986]\n",
      "Cost= 0.42821718505031503\n",
      "[-2.9061853768340895, -1.5801691317686346, 2.4851372952332125]\n",
      "[[ 74.  85. 123.   1.]\n",
      " [ 73.  84. 122.   1.]\n",
      " [ 72.  83. 121.   1.]\n",
      " [ 70.  81. 119.   1.]\n",
      " [ 70.  81. 119.   1.]\n",
      " [ 69.  80. 118.   1.]\n",
      " [ 70.  81. 119.   1.]\n",
      " [ 70.  81. 119.   1.]\n",
      " [ 76.  87. 125.   1.]\n",
      " [ 76.  87. 125.   1.]\n",
      " [ 77.  88. 126.   1.]\n",
      " [ 77.  88. 126.   1.]\n",
      " [ 77.  88. 126.   1.]\n",
      " [ 78.  89. 127.   1.]\n",
      " [ 77.  85. 125.   1.]\n",
      " [193. 189. 148.   0.]\n",
      " [193. 189. 148.   0.]\n",
      " [193. 189. 148.   0.]\n",
      " [192. 188. 147.   0.]\n",
      " [192. 188. 147.   0.]\n",
      " [192. 188. 147.   0.]\n",
      " [194. 190. 149.   0.]\n",
      " [193. 189. 148.   0.]\n",
      " [192. 188. 147.   0.]\n",
      " [193. 189. 148.   0.]\n",
      " [194. 190. 149.   0.]\n",
      " [193. 189. 148.   0.]\n",
      " [193. 189. 148.   0.]\n",
      " [193. 190. 145.   0.]\n",
      " [173. 172. 121.   0.]\n",
      " [167. 167. 113.   0.]\n",
      " [163. 163. 109.   0.]\n",
      " [165. 165. 111.   0.]\n",
      " [165. 165. 111.   0.]\n",
      " [164. 164. 110.   0.]\n",
      " [162. 162. 108.   0.]\n",
      " [163. 163. 109.   0.]\n",
      " [163. 163. 109.   0.]\n",
      " [163. 163. 109.   0.]\n",
      " [163. 163. 109.   0.]\n",
      " [163. 163. 109.   0.]\n",
      " [162. 162. 108.   0.]\n",
      " [162. 162. 108.   0.]\n",
      " [162. 162. 108.   0.]\n",
      " [162. 162. 108.   0.]\n",
      " [162. 161. 105.   0.]\n",
      " [162. 161. 105.   0.]\n",
      " [161. 160. 104.   0.]\n",
      " [161. 160. 104.   0.]\n",
      " [161. 160. 104.   0.]\n",
      " [161. 160. 104.   0.]\n",
      " [161. 159. 105.   0.]\n",
      " [161. 159. 105.   0.]\n",
      " [159. 157. 103.   0.]\n",
      " [ 83.  91. 131.   1.]\n",
      " [ 82.  90. 130.   1.]\n",
      " [ 81.  89. 129.   1.]\n",
      " [157. 155. 101.   0.]\n",
      " [154. 152.  98.   0.]\n",
      " [151. 149.  95.   0.]\n",
      " [149. 147.  93.   0.]\n",
      " [148. 146.  92.   0.]\n",
      " [142. 142.  88.   0.]\n",
      " [141. 141.  87.   0.]\n",
      " [140. 140.  86.   0.]\n",
      " [138. 138.  84.   0.]\n",
      " [ 78.  86. 126.   1.]\n",
      " [ 78.  86. 126.   1.]\n",
      " [ 77.  85. 125.   1.]\n",
      " [ 76.  84. 124.   1.]\n",
      " [ 77.  85. 125.   1.]\n",
      " [ 80.  88. 128.   1.]\n",
      " [ 83.  91. 131.   1.]\n",
      " [ 83.  91. 131.   1.]\n",
      " [ 84.  92. 132.   1.]\n",
      " [ 84.  92. 132.   1.]\n",
      " [ 83.  91. 131.   1.]\n",
      " [ 80.  88. 128.   1.]\n",
      " [ 78.  86. 126.   1.]\n",
      " [ 78.  86. 126.   1.]\n",
      " [ 79.  87. 127.   1.]\n",
      " [159. 157. 103.   0.]\n",
      " [159. 157. 103.   0.]\n",
      " [159. 157. 103.   0.]\n",
      " [159. 157. 103.   0.]\n",
      " [158. 156. 102.   0.]\n",
      " [158. 156. 102.   0.]\n",
      " [158. 156. 102.   0.]\n",
      " [159. 157. 103.   0.]\n",
      " [159. 157. 103.   0.]\n",
      " [159. 157. 103.   0.]\n",
      " [ 78.  86. 126.   1.]\n",
      " [ 79.  87. 127.   1.]\n",
      " [ 81.  89. 129.   1.]\n",
      " [ 82.  90. 130.   1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy Score=  0.5684210526315789\n"
     ]
    }
   ],
   "source": [
    "theta0 = [0, 0, 0]\n",
    "global theta\n",
    "a = 0.1\n",
    "iter = 1000\n",
    "X = data[:, 0:3]\n",
    "theta=Logistic_Regression(X, Y, a, theta0, iter)\n",
    "print(theta)\n",
    "e=np.zeros(shape=(len(data1),1),dtype=np.float64)\n",
    "with open(\"skin_notskin_result.txt\", \"w\") as infile:\n",
    "    for i in range(0,len(data1)):\n",
    "        e[i]=hOFx(theta,X[i])\n",
    "        if e[i]>0.40:\n",
    "            infile.write(str(data1[i])+\"\\t\\t\"+str(1.0)+\"\\n\")\n",
    "            e[i] = 1.0\n",
    "        else:\n",
    "            infile.write(str(data1[i])+\"\\t\\t\"+str(0.0)+ \"\\n\")\n",
    "            e[i] = 0.0\n",
    "tot=0.0\n",
    "for i in range(0,len(data1)):\n",
    "    if Y_test[i]==e[i]:\n",
    "        tot+=1.0\n",
    "print(data1)\n",
    "print(e)\n",
    "print(\"Accuracy Score= \",tot/len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
